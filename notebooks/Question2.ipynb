{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_S5wYu3zkemK"
      },
      "source": [
        "Layer-3 of a deep neural network has three Sigmoid neurons and layer-4 has seven ReLU neurons. You can assume that the output of the layer-3 is random and uniformly distributed in the interval of [0.0, 1.0), that you have to generate using a NumPy method. As explained in the slides, the weights for the layer-4 is initialized using He Normal method and biases are all 0s, that you have to generate using NumPy methods.\n",
        "\n",
        "Write Python code that shows all of these operations and display the output of layer-4. You are NOT expected to use Keras or TensorFlow for this exercise."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_7WsvbckdrH"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYcc3xMskr6b"
      },
      "source": [
        "Generate random output from layer 3 (3 Sigmoid neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYdnQu2ik7bg"
      },
      "outputs": [],
      "source": [
        "output_layer_3 = np.random.rand(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdpgsjWlDYA"
      },
      "source": [
        "Initialize weights for layer 4 (7 ReLU neurons) using He Normal method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVWsZcCMlHsg"
      },
      "outputs": [],
      "source": [
        "weights_layer_4 = np.random.normal(size=(3, 7), scale=np.sqrt(2/3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RogGKsFblLBw"
      },
      "source": [
        "Initialize biases for layer 4 with all 0s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGIN03wPlPpo"
      },
      "outputs": [],
      "source": [
        "biases_layer_4 = np.zeros(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCEl9mOtlTSw"
      },
      "source": [
        "Calculate the output of layer 4 using ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNK1SvFNlX84"
      },
      "outputs": [],
      "source": [
        "output_layer_4 = np.maximum(np.dot(output_layer_3, weights_layer_4) + biases_layer_4, 0)\n",
        "\n",
        "print(\"Output of Layer 4:\",output_layer_4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
